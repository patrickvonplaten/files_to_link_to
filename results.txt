
0: Summary for DeepPavlov/bert-base-bg-cs-pl-ru-cased
--------------------------------------------------
TODO_1: In DeepPavlov/bert-base-bg-cs-pl-ru-cased the eos_token_ids has to be removed
TODO_2: In DeepPavlov/bert-base-bg-cs-pl-ru-cased eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for DeepPavlov/bert-base-bg-cs-pl-ru-cased!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for DeepPavlov/bert-base-bg-cs-pl-ru-cased!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

1: Summary for DeepPavlov/bert-base-cased-conversational
--------------------------------------------------
TODO_1: In DeepPavlov/bert-base-cased-conversational the eos_token_ids has to be removed
TODO_2: In DeepPavlov/bert-base-cased-conversational eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for DeepPavlov/bert-base-cased-conversational!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for DeepPavlov/bert-base-cased-conversational!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

2: Summary for DeepPavlov/bert-base-multilingual-cased-sentence
--------------------------------------------------
TODO_1: In DeepPavlov/bert-base-multilingual-cased-sentence the eos_token_ids has to be removed
TODO_2: In DeepPavlov/bert-base-multilingual-cased-sentence eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for DeepPavlov/bert-base-multilingual-cased-sentence!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for DeepPavlov/bert-base-multilingual-cased-sentence!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

3: Summary for DeepPavlov/rubert-base-cased-conversational
--------------------------------------------------
TODO_1: In DeepPavlov/rubert-base-cased-conversational the eos_token_ids has to be removed
TODO_2: In DeepPavlov/rubert-base-cased-conversational eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for DeepPavlov/rubert-base-cased-conversational!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for DeepPavlov/rubert-base-cased-conversational!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

4: Summary for DeepPavlov/rubert-base-cased-sentence
--------------------------------------------------
TODO_1: In DeepPavlov/rubert-base-cased-sentence the eos_token_ids has to be removed
TODO_2: In DeepPavlov/rubert-base-cased-sentence eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for DeepPavlov/rubert-base-cased-sentence!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for DeepPavlov/rubert-base-cased-sentence!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

5: Summary for DeepPavlov/rubert-base-cased
--------------------------------------------------
TODO_1: In DeepPavlov/rubert-base-cased the eos_token_ids has to be removed
TODO_2: In DeepPavlov/rubert-base-cased eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for DeepPavlov/rubert-base-cased!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for DeepPavlov/rubert-base-cased!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

6: Summary for DrMatters/rubert_cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

7: Summary for Itcast/bert-base-cnc
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

8: Summary for Itcast/cnc_output
--------------------------------------------------
CONF ERROR: Itcast/cnc_output config can not be loaded
Message: Unrecognized model in Itcast/cnc_output. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

9: Summary for ItcastAI/bert_cn_finetuning
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

10: Summary for ItcastAI/bert_finetuning_test
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

11: Summary for KB/albert-base-swedish-cased-alpha
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

12: Summary for KB/albert-base-v2-ner
--------------------------------------------------
TODO_1: In KB/albert-base-v2-ner the eos_token_ids has to be removed
TODO_2: In KB/albert-base-v2-ner eos_token_ids is 0 but default eos_token_id is 3 - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for KB/albert-base-v2-ner!
EOS in Tokenizer: 3 |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for KB/albert-base-v2-ner!
BOS in Tokenizer: 2 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

13: Summary for KB/albert-large-v2-ner
--------------------------------------------------
TODO_1: In KB/albert-large-v2-ner the eos_token_ids has to be removed
TODO_2: In KB/albert-large-v2-ner eos_token_ids is 0 but default eos_token_id is 3 - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for KB/albert-large-v2-ner!
EOS in Tokenizer: 3 |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for KB/albert-large-v2-ner!
BOS in Tokenizer: 2 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

14: Summary for KB/albert-xlarge-v2-ner
--------------------------------------------------
TODO_1: In KB/albert-xlarge-v2-ner the eos_token_ids has to be removed
TODO_2: In KB/albert-xlarge-v2-ner eos_token_ids is 0 but default eos_token_id is 3 - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for KB/albert-xlarge-v2-ner!
EOS in Tokenizer: 3 |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for KB/albert-xlarge-v2-ner!
BOS in Tokenizer: 2 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

15: Summary for KB/bert-base-swedish-cased-alpha
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

16: Summary for KB/bert-base-swedish-cased-ner
--------------------------------------------------
TODO_1: In KB/bert-base-swedish-cased-ner the eos_token_ids has to be removed
TODO_2: In KB/bert-base-swedish-cased-ner eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for KB/bert-base-swedish-cased-ner!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for KB/bert-base-swedish-cased-ner!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

17: Summary for KB/bert-base-swedish-cased-neriob
--------------------------------------------------
TODO_1: In KB/bert-base-swedish-cased-neriob the eos_token_ids has to be removed
TODO_2: In KB/bert-base-swedish-cased-neriob eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for KB/bert-base-swedish-cased-neriob!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for KB/bert-base-swedish-cased-neriob!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

18: Summary for KB/bert-base-swedish-cased
--------------------------------------------------
TODO_1: In KB/bert-base-swedish-cased the eos_token_ids has to be removed
TODO_2: In KB/bert-base-swedish-cased eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for KB/bert-base-swedish-cased!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for KB/bert-base-swedish-cased!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

19: Summary for Musixmatch/umberto-commoncrawl-cased-v1
--------------------------------------------------
--------------------------------------------------
EOS in Tokenizer and Config not equal for Musixmatch/umberto-commoncrawl-cased-v1!
EOS in Tokenizer: 6 |EOS in Config: 2
--------------------------------------------------
BOS in Tokenizer and Config not equal for Musixmatch/umberto-commoncrawl-cased-v1!
BOS in Tokenizer: 5 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

20: Summary for Musixmatch/umberto-wikipedia-uncased-v1
--------------------------------------------------
--------------------------------------------------
EOS in Tokenizer and Config not equal for Musixmatch/umberto-wikipedia-uncased-v1!
EOS in Tokenizer: 6 |EOS in Config: 2
--------------------------------------------------
BOS in Tokenizer and Config not equal for Musixmatch/umberto-wikipedia-uncased-v1!
BOS in Tokenizer: 5 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

21: Summary for Narsil/fr_pretrained
--------------------------------------------------
CONF ERROR: Narsil/fr_pretrained config can not be loaded
Message: Unrecognized model in Narsil/fr_pretrained. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

22: Summary for Narsil/pretrained
--------------------------------------------------
CONF ERROR: Narsil/pretrained config can not be loaded
Message: Unrecognized model in Narsil/pretrained. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

23: Summary for Narsil/pretrained2
--------------------------------------------------
CONF ERROR: Narsil/pretrained2 config can not be loaded
Message: Unrecognized model in Narsil/pretrained2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

24: Summary for SpanBERT/spanbert-base-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

25: Summary for SpanBERT/spanbert-large-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

26: Summary for TurkuNLP/bert-base-finnish-cased-v1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

27: Summary for TurkuNLP/bert-base-finnish-uncased-v1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

28: Summary for absa/bert-lapt-0.1
--------------------------------------------------
TODO_1: In absa/bert-lapt-0.1 the eos_token_ids has to be removed
TODO_2: In absa/bert-lapt-0.1 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for absa/bert-lapt-0.1!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for absa/bert-lapt-0.1!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

29: Summary for absa/bert-rest-0.1
--------------------------------------------------
TODO_1: In absa/bert-rest-0.1 the eos_token_ids has to be removed
TODO_2: In absa/bert-rest-0.1 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for absa/bert-rest-0.1!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for absa/bert-rest-0.1!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

30: Summary for absa/bert-rest-lapt-0.1
--------------------------------------------------
TODO_1: In absa/bert-rest-lapt-0.1 the eos_token_ids has to be removed
TODO_2: In absa/bert-rest-lapt-0.1 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for absa/bert-rest-lapt-0.1!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for absa/bert-rest-lapt-0.1!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

31: Summary for absa/classifier-rest-0.1
--------------------------------------------------
TODO_1: In absa/classifier-rest-0.1 the eos_token_ids has to be removed
TODO_2: In absa/classifier-rest-0.1 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for absa/classifier-rest-0.1!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for absa/classifier-rest-0.1!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

32: Summary for acamara
--------------------------------------------------
CONF ERROR: acamara config can not be loaded
Message: Unrecognized model in acamara. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

33: Summary for adamlin/ClinicalBert_all_notes
--------------------------------------------------
CONF ERROR: adamlin/ClinicalBert_all_notes config can not be loaded
Message: Unrecognized model in adamlin/ClinicalBert_all_notes. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

34: Summary for adamlin/ClinicalBert_disch
--------------------------------------------------
CONF ERROR: adamlin/ClinicalBert_disch config can not be loaded
Message: Unrecognized model in adamlin/ClinicalBert_disch. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

35: Summary for adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers
--------------------------------------------------
CONF ERROR: adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers config can not be loaded
Message: Unrecognized model in adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

36: Summary for adamlin/NCBI_BERT_pubmed_mimic_uncased_large_transformers
--------------------------------------------------
CONF ERROR: adamlin/NCBI_BERT_pubmed_mimic_uncased_large_transformers config can not be loaded
Message: Unrecognized model in adamlin/NCBI_BERT_pubmed_mimic_uncased_large_transformers. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

37: Summary for adamlin/bert-distil-chinese
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

38: Summary for af-ai-center/bert-base-swedish-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

39: Summary for af-ai-center/bert-large-swedish-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

40: Summary for ahotrod/albert_xxlargev1_squad2_512
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

41: Summary for ahotrod/xlnet_large_squad2_512
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

42: Summary for albert-base
--------------------------------------------------
CONF ERROR: albert-base config can not be loaded
Message: Can't load 'albert-base'. Make sure that:

- 'albert-base' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'albert-base' is the correct path to a directory containing a 'config.json' file


==================================================

43: Summary for albert-base-v2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

44: Summary for albert-large
--------------------------------------------------
CONF ERROR: albert-large config can not be loaded
Message: Can't load 'albert-large'. Make sure that:

- 'albert-large' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'albert-large' is the correct path to a directory containing a 'config.json' file


==================================================

45: Summary for albert-large-v2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

46: Summary for albert-xlarge
--------------------------------------------------
CONF ERROR: albert-xlarge config can not be loaded
Message: Can't load 'albert-xlarge'. Make sure that:

- 'albert-xlarge' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'albert-xlarge' is the correct path to a directory containing a 'config.json' file


==================================================

47: Summary for albert-xlarge-v2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

48: Summary for albert-xxlarge
--------------------------------------------------
CONF ERROR: albert-xxlarge config can not be loaded
Message: Can't load 'albert-xxlarge'. Make sure that:

- 'albert-xxlarge' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'albert-xxlarge' is the correct path to a directory containing a 'config.json' file


==================================================

49: Summary for albert-xxlarge-v2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

50: Summary for allenai/scibert_scivocab_cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

51: Summary for allenai/scibert_scivocab_uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

52: Summary for asafaya/bert-base-arabic
--------------------------------------------------
TODO_1: In asafaya/bert-base-arabic the eos_token_ids has to be removed
TODO_2: In asafaya/bert-base-arabic eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for asafaya/bert-base-arabic!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for asafaya/bert-base-arabic!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

53: Summary for aubmindlab/bert-base-arabert
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for aubmindlab/bert-base-arabert!
PAD in Tokenizer: 29757 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

54: Summary for aubmindlab/bert-base-arabertv01
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for aubmindlab/bert-base-arabertv01!
PAD in Tokenizer: 17029 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

55: Summary for bert-base-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

56: Summary for bert-base-cased-finetuned-mrpc
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

57: Summary for bert-base-chinese
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

58: Summary for bert-base-german-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

59: Summary for bert-base-german-dbmdz-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

60: Summary for bert-base-german-dbmdz-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

61: Summary for bert-base-multilingual-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

62: Summary for bert-base-multilingual-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

63: Summary for bert-base-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

64: Summary for bert-large-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

65: Summary for bert-large-cased-whole-word-masking
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

66: Summary for bert-large-cased-whole-word-masking-finetuned-squad
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

67: Summary for bert-large-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

68: Summary for bert-large-uncased-whole-word-masking
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

69: Summary for bert-large-uncased-whole-word-masking-finetuned-squad
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

70: Summary for binwang/bert-base-nli-stsb
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

71: Summary for binwang/bert-base-nli
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

72: Summary for binwang/bert-base-uncased
--------------------------------------------------
TODO_1: In binwang/bert-base-uncased the eos_token_ids has to be removed
TODO_2: In binwang/bert-base-uncased eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for binwang/bert-base-uncased!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for binwang/bert-base-uncased!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

73: Summary for binwang/bert-large-nli-stsb
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

74: Summary for binwang/bert-large-nli
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

75: Summary for binwang/roberta-base
--------------------------------------------------
TODO_1: In binwang/roberta-base the eos_token_ids has to be removed
TODO_2: In binwang/roberta-base eos_token_ids is 0 but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for binwang/roberta-base!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for binwang/roberta-base!
EOS in Tokenizer: 2 |EOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

76: Summary for binwang/xlnet-base-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

77: Summary for camembert-base
--------------------------------------------------
--------------------------------------------------
EOS in Tokenizer and Config not equal for camembert-base!
EOS in Tokenizer: 6 |EOS in Config: 2
--------------------------------------------------
BOS in Tokenizer and Config not equal for camembert-base!
BOS in Tokenizer: 5 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

78: Summary for canwenxu/BERT-of-Theseus-MNLI
--------------------------------------------------
TODO_1: In canwenxu/BERT-of-Theseus-MNLI the eos_token_ids has to be removed
TODO_2: In canwenxu/BERT-of-Theseus-MNLI eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for canwenxu/BERT-of-Theseus-MNLI!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for canwenxu/BERT-of-Theseus-MNLI!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

79: Summary for bert-base-japanese-char
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

80: Summary for bert-base-japanese-char-whole-word-masking
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

81: Summary for bert-base-japanese
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

82: Summary for bert-base-japanese-whole-word-masking
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

83: Summary for clue/albert_chinese_small
--------------------------------------------------
TOK ERROR: clue/albert_chinese_small tokenizer can not be loaded
Message: Model name 'clue/albert_chinese_small' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'clue/albert_chinese_small' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

84: Summary for clue/albert_chinese_tiny
--------------------------------------------------
TOK ERROR: clue/albert_chinese_tiny tokenizer can not be loaded
Message: Model name 'clue/albert_chinese_tiny' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'clue/albert_chinese_tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

85: Summary for clue/roberta_chinese_3L312_clue_tiny
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_3L312_clue_tiny tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_3L312_clue_tiny' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_3L312_clue_tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

86: Summary for clue/roberta_chinese_3L768_clue_tiny
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_3L768_clue_tiny tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_3L768_clue_tiny' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_3L768_clue_tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

87: Summary for clue/roberta_chinese_base
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_base tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_base' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_base' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

88: Summary for clue/roberta_chinese_clue_large
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_clue_large tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_clue_large' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_clue_large' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

89: Summary for clue/roberta_chinese_clue_tiny
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_clue_tiny tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_clue_tiny' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_clue_tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

90: Summary for clue/roberta_chinese_large
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_large tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_large' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_large' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

91: Summary for clue/roberta_chinese_pair_large
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_pair_large tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_pair_large' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_pair_large' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

92: Summary for clue/roberta_chinese_pair_tiny
--------------------------------------------------
TOK ERROR: clue/roberta_chinese_pair_tiny tokenizer can not be loaded
Message: Model name 'clue/roberta_chinese_pair_tiny' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'clue/roberta_chinese_pair_tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

93: Summary for clue/xlnet_chinese_large
--------------------------------------------------
TODO_1: In clue/xlnet_chinese_large the eos_token_ids has to be removed
TODO_2: In clue/xlnet_chinese_large eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for clue/xlnet_chinese_large!
PAD in Tokenizer: 5 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for clue/xlnet_chinese_large!
EOS in Tokenizer: 2 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for clue/xlnet_chinese_large!
BOS in Tokenizer: 1 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

94: Summary for dbmdz/bert-base-cased-finetuned-conll03-english
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

95: Summary for dbmdz/bert-base-german-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

96: Summary for dbmdz/bert-base-german-europeana-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

97: Summary for dbmdz/bert-base-german-europeana-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

98: Summary for dbmdz/bert-base-german-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

99: Summary for dbmdz/bert-base-italian-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

100: Summary for dbmdz/bert-base-italian-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

101: Summary for dbmdz/bert-base-italian-xxl-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

102: Summary for dbmdz/bert-base-italian-xxl-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

103: Summary for dbmdz/bert-base-turkish-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

104: Summary for dbmdz/bert-large-cased-finetuned-conll03-english
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

105: Summary for dbmdz/distilbert-base-turkish-cased
--------------------------------------------------
TODO_1: In dbmdz/distilbert-base-turkish-cased the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for dbmdz/distilbert-base-turkish-cased!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

106: Summary for dccuchile/bert-base-spanish-wwm-cased
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for dccuchile/bert-base-spanish-wwm-cased!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

107: Summary for dccuchile/bert-base-spanish-wwm-uncased
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for dccuchile/bert-base-spanish-wwm-uncased!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

108: Summary for dccuchile/cased
--------------------------------------------------
CONF ERROR: dccuchile/cased config can not be loaded
Message: Unrecognized model in dccuchile/cased. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

109: Summary for dccuchile/uncased
--------------------------------------------------
CONF ERROR: dccuchile/uncased config can not be loaded
Message: Unrecognized model in dccuchile/uncased. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

110: Summary for deepset/bert-base-cased-squad2
--------------------------------------------------
TODO_1: In deepset/bert-base-cased-squad2 the eos_token_ids has to be removed
TODO_2: In deepset/bert-base-cased-squad2 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for deepset/bert-base-cased-squad2!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for deepset/bert-base-cased-squad2!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

111: Summary for deepset/bert-base-german-cased-hatespeech-GermEval18Coarse
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

112: Summary for deepset/bert-large-uncased-whole-word-masking-squad2
--------------------------------------------------
TODO_1: In deepset/bert-large-uncased-whole-word-masking-squad2 the eos_token_ids has to be removed
TODO_2: In deepset/bert-large-uncased-whole-word-masking-squad2 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for deepset/bert-large-uncased-whole-word-masking-squad2!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for deepset/bert-large-uncased-whole-word-masking-squad2!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

113: Summary for deepset/roberta-base-squad2
--------------------------------------------------
TODO_1: In deepset/roberta-base-squad2 the eos_token_ids has to be removed
TODO_2: In deepset/roberta-base-squad2 eos_token_ids is 0 but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for deepset/roberta-base-squad2!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for deepset/roberta-base-squad2!
EOS in Tokenizer: 2 |EOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

114: Summary for deepset/sentence_bert
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

115: Summary for denpa92/bert-base-cantonese
--------------------------------------------------
TOK ERROR: denpa92/bert-base-cantonese tokenizer can not be loaded
Message: Model name 'denpa92/bert-base-cantonese' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'denpa92/bert-base-cantonese' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

116: Summary for distilbert-base-cased
--------------------------------------------------
TODO_1: In distilbert-base-cased the eos_token_ids has to be removed
TODO_2: In distilbert-base-cased eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for distilbert-base-cased!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for distilbert-base-cased!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

117: Summary for distilbert-base-cased-distilled-squad
--------------------------------------------------
TODO_1: In distilbert-base-cased-distilled-squad the eos_token_ids has to be removed
TODO_2: In distilbert-base-cased-distilled-squad eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for distilbert-base-cased-distilled-squad!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for distilbert-base-cased-distilled-squad!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

118: Summary for distilbert-base-german-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

119: Summary for distilbert-base-multilingual-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

120: Summary for distilbert-base-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

121: Summary for distilbert-base-uncased-distilled-squad
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

122: Summary for distilbert-base-uncased-finetuned-sst-2-english
--------------------------------------------------
TOK ERROR: distilbert-base-uncased-finetuned-sst-2-english tokenizer can not be loaded
Message: Model name 'distilbert-base-uncased-finetuned-sst-2-english' was not found in tokenizers model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). We assumed 'distilbert-base-uncased-finetuned-sst-2-english' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

123: Summary for distilgpt2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

124: Summary for distilroberta-base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

125: Summary for djstrong/bg_cs_pl_ru_cased_L-12_H-768_A-12
--------------------------------------------------
CONF ERROR: djstrong/bg_cs_pl_ru_cased_L-12_H-768_A-12 config can not be loaded
Message: Unrecognized model in djstrong/bg_cs_pl_ru_cased_L-12_H-768_A-12. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

126: Summary for dkleczek/bert-base-polish-uncased-v1
--------------------------------------------------
TODO_1: In dkleczek/bert-base-polish-uncased-v1 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for dkleczek/bert-base-polish-uncased-v1!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

127: Summary for emilyalsentzer/Bio_ClinicalBERT
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

128: Summary for emilyalsentzer/Bio_Discharge_Summary_BERT
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

129: Summary for facebook/bart-large-cnn
--------------------------------------------------
TOK ERROR: facebook/bart-large-cnn tokenizer can not be loaded
Message: Model name 'facebook/bart-large-cnn' was not found in tokenizers model name list (bart-large, bart-large-mnli, bart-large-cnn). We assumed 'facebook/bart-large-cnn' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

130: Summary for facebook/bart-large-mnli
--------------------------------------------------
TOK ERROR: facebook/bart-large-mnli tokenizer can not be loaded
Message: Model name 'facebook/bart-large-mnli' was not found in tokenizers model name list (bart-large, bart-large-mnli, bart-large-cnn). We assumed 'facebook/bart-large-mnli' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

131: Summary for facebook/bart-large-xsum
--------------------------------------------------
TOK ERROR: facebook/bart-large-xsum tokenizer can not be loaded
Message: Model name 'facebook/bart-large-xsum' was not found in tokenizers model name list (bart-large, bart-large-mnli, bart-large-cnn). We assumed 'facebook/bart-large-xsum' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

132: Summary for facebook/bart-large
--------------------------------------------------
TOK ERROR: facebook/bart-large tokenizer can not be loaded
Message: Model name 'facebook/bart-large' was not found in tokenizers model name list (bart-large, bart-large-mnli, bart-large-cnn). We assumed 'facebook/bart-large' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

133: Summary for flaubert/flaubert_base_cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

134: Summary for flaubert/flaubert_base_uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

135: Summary for flaubert/flaubert_large_cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

136: Summary for flaubert/flaubert_small_cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

137: Summary for fmikaelian/camembert-base-fquad
--------------------------------------------------
TODO_1: In fmikaelian/camembert-base-fquad the eos_token_ids has to be removed
TODO_2: In fmikaelian/camembert-base-fquad eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for fmikaelian/camembert-base-fquad!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for fmikaelian/camembert-base-fquad!
EOS in Tokenizer: 6 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for fmikaelian/camembert-base-fquad!
BOS in Tokenizer: 5 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

138: Summary for fmikaelian/camembert-base-squad
--------------------------------------------------
TODO_1: In fmikaelian/camembert-base-squad the eos_token_ids has to be removed
TODO_2: In fmikaelian/camembert-base-squad eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for fmikaelian/camembert-base-squad!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for fmikaelian/camembert-base-squad!
EOS in Tokenizer: 6 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for fmikaelian/camembert-base-squad!
BOS in Tokenizer: 5 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

139: Summary for fmikaelian/flaubert-base-uncased-squad
--------------------------------------------------
TODO_1: In fmikaelian/flaubert-base-uncased-squad the eos_token_ids has to be removed
TODO_2: In fmikaelian/flaubert-base-uncased-squad eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for fmikaelian/flaubert-base-uncased-squad!
PAD in Tokenizer: 2 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for fmikaelian/flaubert-base-uncased-squad!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

140: Summary for google/bert_uncased_L-10_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

141: Summary for google/bert_uncased_L-10_H-256_A-4
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

142: Summary for google/bert_uncased_L-10_H-512_A-8
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

143: Summary for google/bert_uncased_L-10_H-768_A-12
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

144: Summary for google/bert_uncased_L-12_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

145: Summary for google/bert_uncased_L-12_H-256_A-4
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

146: Summary for google/bert_uncased_L-12_H-512_A-8
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

147: Summary for google/bert_uncased_L-12_H-768_A-12
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

148: Summary for google/bert_uncased_L-2_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

149: Summary for google/bert_uncased_L-2_H-256_A-4
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

150: Summary for google/bert_uncased_L-2_H-512_A-8
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

151: Summary for google/bert_uncased_L-2_H-768_A-12
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

152: Summary for google/bert_uncased_L-4_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

153: Summary for google/bert_uncased_L-4_H-256_A-4
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

154: Summary for google/bert_uncased_L-4_H-512_A-8
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

155: Summary for google/bert_uncased_L-4_H-768_A-12
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

156: Summary for google/bert_uncased_L-6_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

157: Summary for google/bert_uncased_L-6_H-256_A-4
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

158: Summary for google/bert_uncased_L-6_H-512_A-8
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

159: Summary for google/bert_uncased_L-6_H-768_A-12
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

160: Summary for google/bert_uncased_L-8_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

161: Summary for google/bert_uncased_L-8_H-256_A-4
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

162: Summary for google/bert_uncased_L-8_H-512_A-8
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

163: Summary for google/bert_uncased_L-8_H-768_A-12
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

164: Summary for google/electra-base
--------------------------------------------------
CONF ERROR: google/electra-base config can not be loaded
Message: Unrecognized model in google/electra-base. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

165: Summary for google/electra-large
--------------------------------------------------
CONF ERROR: google/electra-large config can not be loaded
Message: Unrecognized model in google/electra-large. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

166: Summary for google/electra-small
--------------------------------------------------
CONF ERROR: google/electra-small config can not be loaded
Message: Unrecognized model in google/electra-small. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

167: Summary for gpt2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

168: Summary for gpt2-large
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

169: Summary for gpt2-medium
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

170: Summary for gpt2-xl
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

171: Summary for guolingqi/bert_base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

172: Summary for henryk/bert-base-multilingual-cased-finetuned-dutch-squad1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

173: Summary for henryk/bert-base-multilingual-cased-finetuned-dutch-squad2
--------------------------------------------------
TODO_1: In henryk/bert-base-multilingual-cased-finetuned-dutch-squad2 the eos_token_ids has to be removed
TODO_2: In henryk/bert-base-multilingual-cased-finetuned-dutch-squad2 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for henryk/bert-base-multilingual-cased-finetuned-dutch-squad2!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for henryk/bert-base-multilingual-cased-finetuned-dutch-squad2!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

174: Summary for hfl/chinese-bert-wwm-ext
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

175: Summary for hfl/chinese-bert-wwm
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

176: Summary for hfl/chinese-roberta-wwm-ext-large
--------------------------------------------------
TOK ERROR: hfl/chinese-roberta-wwm-ext-large tokenizer can not be loaded
Message: expected str, bytes or os.PathLike object, not NoneType
==================================================

177: Summary for hfl/chinese-roberta-wwm-ext
--------------------------------------------------
TOK ERROR: hfl/chinese-roberta-wwm-ext tokenizer can not be loaded
Message: expected str, bytes or os.PathLike object, not NoneType
==================================================

178: Summary for hfl/chinese-xlnet-base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

179: Summary for hfl/chinese-xlnet-mid
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

180: Summary for hfl/rbt3
--------------------------------------------------
CONF ERROR: hfl/rbt3 config can not be loaded
Message: Unrecognized model in hfl/rbt3. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

181: Summary for hfl/rbtl3
--------------------------------------------------
CONF ERROR: hfl/rbtl3 config can not be loaded
Message: Unrecognized model in hfl/rbtl3. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

182: Summary for huggingface/CodeBERTa-language-id
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

183: Summary for huggingface/CodeBERTa-small-v1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

184: Summary for huseinzol05/bert-base-bahasa-cased
--------------------------------------------------
TOK ERROR: huseinzol05/bert-base-bahasa-cased tokenizer can not be loaded
Message: stat: path should be string, bytes, os.PathLike or integer, not NoneType
==================================================

185: Summary for idb-ita/gilberto-uncased-from-camembert
--------------------------------------------------
TODO_1: In idb-ita/gilberto-uncased-from-camembert the eos_token_ids has to be removed
TODO_2: In idb-ita/gilberto-uncased-from-camembert eos_token_ids is 0 but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for idb-ita/gilberto-uncased-from-camembert!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for idb-ita/gilberto-uncased-from-camembert!
EOS in Tokenizer: 6 |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for idb-ita/gilberto-uncased-from-camembert!
BOS in Tokenizer: 5 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

186: Summary for irvingpop/dreambank
--------------------------------------------------
TODO_1: In irvingpop/dreambank the eos_token_ids has to be removed
TODO_2: In irvingpop/dreambank eos_token_ids is None but default eos_token_id is 50256 - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for irvingpop/dreambank!
EOS in Tokenizer: 50256 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for irvingpop/dreambank!
BOS in Tokenizer: 50256 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

187: Summary for iuliaturc/bert_uncased_L-2_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

188: Summary for ixa-ehu/berteus-base-cased
--------------------------------------------------
TODO_1: In ixa-ehu/berteus-base-cased the eos_token_ids has to be removed
TODO_2: In ixa-ehu/berteus-base-cased eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for ixa-ehu/berteus-base-cased!
PAD in Tokenizer: 3 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for ixa-ehu/berteus-base-cased!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for ixa-ehu/berteus-base-cased!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

189: Summary for jannesg/bertsson
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

190: Summary for jplu/tf-camembert-base
--------------------------------------------------
--------------------------------------------------
EOS in Tokenizer and Config not equal for jplu/tf-camembert-base!
EOS in Tokenizer: 6 |EOS in Config: 2
--------------------------------------------------
BOS in Tokenizer and Config not equal for jplu/tf-camembert-base!
BOS in Tokenizer: 5 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

191: Summary for jplu/tf-flaubert-base-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

192: Summary for jplu/tf-flaubert-base-uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

193: Summary for jplu/tf-flaubert-large-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

194: Summary for jplu/tf-flaubert-small-cased
--------------------------------------------------
TOK ERROR: jplu/tf-flaubert-small-cased tokenizer can not be loaded
Message: Model name 'jplu/tf-flaubert-small-cased' was not found in tokenizers model name list (flaubert-small-cased, flaubert-base-uncased, flaubert-base-cased, flaubert-large-cased). We assumed 'jplu/tf-flaubert-small-cased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

195: Summary for jplu/tf-xlm-roberta-base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

196: Summary for jplu/tf-xlm-roberta-large
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

197: Summary for julien-c/EsperBERTo-small-pos
--------------------------------------------------
TODO_1: In julien-c/EsperBERTo-small-pos the eos_token_ids has to be removed
TODO_2: In julien-c/EsperBERTo-small-pos eos_token_ids is 0 but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for julien-c/EsperBERTo-small-pos!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for julien-c/EsperBERTo-small-pos!
EOS in Tokenizer: 2 |EOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

198: Summary for julien-c/EsperBERTo-small
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

199: Summary for julien-c/bert-xsmall-dummy
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

200: Summary for julien-c/dummy-unknown
--------------------------------------------------
TODO_1: In julien-c/dummy-unknown the eos_token_ids has to be removed
TODO_2: In julien-c/dummy-unknown eos_token_ids is 0 but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for julien-c/dummy-unknown!
PAD in Tokenizer: 19 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for julien-c/dummy-unknown!
EOS in Tokenizer: 19 |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for julien-c/dummy-unknown!
BOS in Tokenizer: 19 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

201: Summary for ktrapeznikov/albert-xlarge-v2-squad-v2
--------------------------------------------------
TODO_1: In ktrapeznikov/albert-xlarge-v2-squad-v2 the eos_token_ids has to be removed
TODO_2: In ktrapeznikov/albert-xlarge-v2-squad-v2 eos_token_ids is 0 but default eos_token_id is 3 - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for ktrapeznikov/albert-xlarge-v2-squad-v2!
EOS in Tokenizer: 3 |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for ktrapeznikov/albert-xlarge-v2-squad-v2!
BOS in Tokenizer: 2 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

202: Summary for lonePatient/albert_chinese_small
--------------------------------------------------
TOK ERROR: lonePatient/albert_chinese_small tokenizer can not be loaded
Message: Model name 'lonePatient/albert_chinese_small' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'lonePatient/albert_chinese_small' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

203: Summary for lonePatient/roberta_chinese_clue_tiny
--------------------------------------------------
TOK ERROR: lonePatient/roberta_chinese_clue_tiny tokenizer can not be loaded
Message: Model name 'lonePatient/roberta_chinese_clue_tiny' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'lonePatient/roberta_chinese_clue_tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

204: Summary for lvwerra/gpt2-medium-taboo
--------------------------------------------------
TODO_1: In lvwerra/gpt2-medium-taboo the eos_token_ids has to be removed
TODO_2: In lvwerra/gpt2-medium-taboo eos_token_ids is 0 but default eos_token_id is 50256 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for lvwerra/gpt2-medium-taboo!
PAD in Tokenizer: None | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for lvwerra/gpt2-medium-taboo!
EOS in Tokenizer: 50256 |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for lvwerra/gpt2-medium-taboo!
BOS in Tokenizer: 50256 | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

205: Summary for lysandre/arxiv-nlp
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

206: Summary for lysandre/arxiv
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

207: Summary for m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

208: Summary for m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alberto
--------------------------------------------------
TOK ERROR: m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alberto tokenizer can not be loaded
Message: Model name 'm-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alberto' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'm-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alberto' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

209: Summary for marrrcin/PolBERTa-base-polish-cased-v1
--------------------------------------------------
TODO_1: In marrrcin/PolBERTa-base-polish-cased-v1 the eos_token_ids has to be removed
TODO_2: In marrrcin/PolBERTa-base-polish-cased-v1 eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for marrrcin/PolBERTa-base-polish-cased-v1!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for marrrcin/PolBERTa-base-polish-cased-v1!
EOS in Tokenizer: 2 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for marrrcin/PolBERTa-base-polish-cased-v1!
BOS in Tokenizer: 0 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

210: Summary for microsoft/DialoGPT-large
--------------------------------------------------
TODO_1: In microsoft/DialoGPT-large the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for microsoft/DialoGPT-large!
PAD in Tokenizer: None | PAD in Config: 50256
--------------------------------------------------
Config needs change!
==================================================

211: Summary for microsoft/DialoGPT-medium
--------------------------------------------------
TODO_1: In microsoft/DialoGPT-medium the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for microsoft/DialoGPT-medium!
PAD in Tokenizer: None | PAD in Config: 50256
--------------------------------------------------
Config needs change!
==================================================

212: Summary for microsoft/DialoGPT-small
--------------------------------------------------
TODO_1: In microsoft/DialoGPT-small the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for microsoft/DialoGPT-small!
PAD in Tokenizer: None | PAD in Config: 50256
--------------------------------------------------
Config needs change!
==================================================

213: Summary for monologg/biobert_v1.0_pubmed_pmc
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

214: Summary for monologg/biobert_v1.1_pubmed
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

215: Summary for monologg/distilkobert
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for monologg/distilkobert!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

216: Summary for monologg/kobert-lm
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for monologg/kobert-lm!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

217: Summary for monologg/kobert
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for monologg/kobert!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

218: Summary for mrm8488/CodeBERTaPy
--------------------------------------------------
TODO_1: In mrm8488/CodeBERTaPy the eos_token_ids has to be removed
TODO_2: In mrm8488/CodeBERTaPy eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/CodeBERTaPy!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for mrm8488/CodeBERTaPy!
EOS in Tokenizer: 2 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for mrm8488/CodeBERTaPy!
BOS in Tokenizer: 0 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

219: Summary for mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es
--------------------------------------------------
TODO_1: In mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es the eos_token_ids has to be removed
TODO_2: In mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

220: Summary for mrm8488/bert-mini-finetuned-squadv2
--------------------------------------------------
TODO_1: In mrm8488/bert-mini-finetuned-squadv2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-mini-finetuned-squadv2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

221: Summary for mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp
--------------------------------------------------
TODO_1: In mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

222: Summary for mrm8488/bert-multi-cased-finetuned-xquadv1
--------------------------------------------------
TODO_1: In mrm8488/bert-multi-cased-finetuned-xquadv1 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-multi-cased-finetuned-xquadv1!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

223: Summary for mrm8488/bert-multi-uncased-finetuned-xquadv1
--------------------------------------------------
TODO_1: In mrm8488/bert-multi-uncased-finetuned-xquadv1 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-multi-uncased-finetuned-xquadv1!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

224: Summary for mrm8488/bert-small-finetuned-squadv2
--------------------------------------------------
TODO_1: In mrm8488/bert-small-finetuned-squadv2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-small-finetuned-squadv2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

225: Summary for mrm8488/bert-spanish-cased-finedtuned-ner
--------------------------------------------------
TODO_1: In mrm8488/bert-spanish-cased-finedtuned-ner the eos_token_ids has to be removed
TODO_2: In mrm8488/bert-spanish-cased-finedtuned-ner eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finedtuned-ner!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finedtuned-ner!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finedtuned-ner!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

226: Summary for mrm8488/bert-spanish-cased-finetuned-ner
--------------------------------------------------
TODO_1: In mrm8488/bert-spanish-cased-finetuned-ner the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finetuned-ner!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

227: Summary for mrm8488/bert-spanish-cased-finetuned-pos-16-tags
--------------------------------------------------
TODO_1: In mrm8488/bert-spanish-cased-finetuned-pos-16-tags the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finetuned-pos-16-tags!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

228: Summary for mrm8488/bert-spanish-cased-finetuned-pos
--------------------------------------------------
TODO_1: In mrm8488/bert-spanish-cased-finetuned-pos the eos_token_ids has to be removed
TODO_2: In mrm8488/bert-spanish-cased-finetuned-pos eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finetuned-pos!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finetuned-pos!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for mrm8488/bert-spanish-cased-finetuned-pos!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

229: Summary for mrm8488/bert-tiny-2-finetuned-squadv2
--------------------------------------------------
TODO_1: In mrm8488/bert-tiny-2-finetuned-squadv2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-tiny-2-finetuned-squadv2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

230: Summary for mrm8488/bert-tiny-3-finetuned-squadv2
--------------------------------------------------
TODO_1: In mrm8488/bert-tiny-3-finetuned-squadv2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-tiny-3-finetuned-squadv2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

231: Summary for mrm8488/bert-tiny-finetuned-squadv2
--------------------------------------------------
TODO_1: In mrm8488/bert-tiny-finetuned-squadv2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/bert-tiny-finetuned-squadv2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

232: Summary for mrm8488/bert-uncased-finetuned-qnli
--------------------------------------------------
TOK ERROR: mrm8488/bert-uncased-finetuned-qnli tokenizer can not be loaded
Message: Model name 'mrm8488/bert-uncased-finetuned-qnli' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'mrm8488/bert-uncased-finetuned-qnli' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

233: Summary for mrm8488/codeBERTaJS
--------------------------------------------------
TODO_1: In mrm8488/codeBERTaJS the eos_token_ids has to be removed
TODO_2: In mrm8488/codeBERTaJS eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/codeBERTaJS!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for mrm8488/codeBERTaJS!
EOS in Tokenizer: 2 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for mrm8488/codeBERTaJS!
BOS in Tokenizer: 0 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

234: Summary for mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es
--------------------------------------------------
TODO_1: In mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es the eos_token_ids has to be removed
TODO_2: In mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es!
PAD in Tokenizer: 1 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

235: Summary for mrm8488/spanbert-finetuned-squadv1
--------------------------------------------------
TODO_1: In mrm8488/spanbert-finetuned-squadv1 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/spanbert-finetuned-squadv1!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

236: Summary for mrm8488/spanbert-finetuned-squadv2
--------------------------------------------------
TODO_1: In mrm8488/spanbert-finetuned-squadv2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/spanbert-finetuned-squadv2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

237: Summary for mrm8488/xlm-multi-finetuned-xquadv1
--------------------------------------------------
TODO_1: In mrm8488/xlm-multi-finetuned-xquadv1 the eos_token_ids has to be removed
--------------------------------------------------
==================================================

238: Summary for neuralmind/bert-base-portuguese-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

239: Summary for neuralmind/bert-large-portuguese-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

240: Summary for nlpaueb/bert-base-greek-uncased-v1
--------------------------------------------------
TODO_1: In nlpaueb/bert-base-greek-uncased-v1 the eos_token_ids has to be removed
TODO_2: In nlpaueb/bert-base-greek-uncased-v1 eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for nlpaueb/bert-base-greek-uncased-v1!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for nlpaueb/bert-base-greek-uncased-v1!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

241: Summary for nlptown/bert-base-multilingual-uncased-sentiment
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

242: Summary for oda/music5
--------------------------------------------------
CONF ERROR: oda/music5 config can not be loaded
Message: Unrecognized model in oda/music5. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

243: Summary for openai-gpt
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

244: Summary for pdelobelle/robBERT-base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

245: Summary for pdelobelle/robBERT-dutch-books
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

246: Summary for pertschuk/albert-base-quora-classifier
--------------------------------------------------
TOK ERROR: pertschuk/albert-base-quora-classifier tokenizer can not be loaded
Message: Model name 'pertschuk/albert-base-quora-classifier' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'pertschuk/albert-base-quora-classifier' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

247: Summary for pertschuk/albert-base-squad-classifier-ms
--------------------------------------------------
TOK ERROR: pertschuk/albert-base-squad-classifier-ms tokenizer can not be loaded
Message: Model name 'pertschuk/albert-base-squad-classifier-ms' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'pertschuk/albert-base-squad-classifier-ms' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

248: Summary for pertschuk/albert-base-squad-classifier
--------------------------------------------------
TOK ERROR: pertschuk/albert-base-squad-classifier tokenizer can not be loaded
Message: Model name 'pertschuk/albert-base-squad-classifier' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'pertschuk/albert-base-squad-classifier' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

249: Summary for pertschuk/albert-large-intent-v2
--------------------------------------------------
TODO_1: In pertschuk/albert-large-intent-v2 the eos_token_ids has to be removed
TODO_2: In pertschuk/albert-large-intent-v2 eos_token_ids is None but default eos_token_id is 3 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for pertschuk/albert-large-intent-v2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for pertschuk/albert-large-intent-v2!
EOS in Tokenizer: 3 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for pertschuk/albert-large-intent-v2!
BOS in Tokenizer: 2 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

250: Summary for pertschuk/albert-large-intent-v3
--------------------------------------------------
TODO_1: In pertschuk/albert-large-intent-v3 the eos_token_ids has to be removed
TODO_2: In pertschuk/albert-large-intent-v3 eos_token_ids is None but default eos_token_id is 3 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for pertschuk/albert-large-intent-v3!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for pertschuk/albert-large-intent-v3!
EOS in Tokenizer: 3 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for pertschuk/albert-large-intent-v3!
BOS in Tokenizer: 2 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

251: Summary for pertschuk/bert-large-uncased-msmarco
--------------------------------------------------
TODO_1: In pertschuk/bert-large-uncased-msmarco the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for pertschuk/bert-large-uncased-msmarco!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

252: Summary for ponteineptique/latin-classical-small
--------------------------------------------------
TODO_1: In ponteineptique/latin-classical-small the eos_token_ids has to be removed
TODO_2: In ponteineptique/latin-classical-small eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for ponteineptique/latin-classical-small!
PAD in Tokenizer: 2 | PAD in Config: 0
--------------------------------------------------
EOS in Tokenizer and Config not equal for ponteineptique/latin-classical-small!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

253: Summary for radha1258/save
--------------------------------------------------
CONF ERROR: radha1258/save config can not be loaded
Message: Unrecognized model in radha1258/save. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

254: Summary for replydotai/albert-xxlarge-v1-finetuned-squad2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

255: Summary for roberta-base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

256: Summary for roberta-base-openai-detector
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

257: Summary for roberta-large
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

258: Summary for roberta-large-mnli
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

259: Summary for roberta-large-openai-detector
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

260: Summary for severinsimmler/german-press-bert
--------------------------------------------------
TODO_1: In severinsimmler/german-press-bert the eos_token_ids has to be removed
TODO_2: In severinsimmler/german-press-bert eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for severinsimmler/german-press-bert!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for severinsimmler/german-press-bert!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

261: Summary for severinsimmler/literary-german-bert
--------------------------------------------------
TODO_1: In severinsimmler/literary-german-bert the eos_token_ids has to be removed
TODO_2: In severinsimmler/literary-german-bert eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for severinsimmler/literary-german-bert!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for severinsimmler/literary-german-bert!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

262: Summary for srush/bert_uncased_L-2_H-128_A-2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

263: Summary for subbareddyiiit/iiit
--------------------------------------------------
CONF ERROR: subbareddyiiit/iiit config can not be loaded
Message: Unrecognized model in subbareddyiiit/iiit. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

264: Summary for subbareddyiiit/music_cog
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

265: Summary for subbareddyiiit/tftelugu
--------------------------------------------------
CONF ERROR: subbareddyiiit/tftelugu config can not be loaded
Message: Unrecognized model in subbareddyiiit/tftelugu. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

266: Summary for t5-11b
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

267: Summary for t5-3b
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

268: Summary for t5-base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

269: Summary for t5-large
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

270: Summary for t5-small
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

271: Summary for transfo-xl-wt103
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

272: Summary for twmkn9/albert-base-v2-squad2
--------------------------------------------------
TODO_1: In twmkn9/albert-base-v2-squad2 the eos_token_ids has to be removed
TODO_2: In twmkn9/albert-base-v2-squad2 eos_token_ids is None but default eos_token_id is 3 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for twmkn9/albert-base-v2-squad2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for twmkn9/albert-base-v2-squad2!
EOS in Tokenizer: 3 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for twmkn9/albert-base-v2-squad2!
BOS in Tokenizer: 2 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

273: Summary for twmkn9/bert-base-uncased-squad2
--------------------------------------------------
TODO_1: In twmkn9/bert-base-uncased-squad2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for twmkn9/bert-base-uncased-squad2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

274: Summary for unilm-base-cased
--------------------------------------------------
CONF ERROR: unilm-base-cased config can not be loaded
Message: Unrecognized model in unilm-base-cased. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

275: Summary for unilm-large-cased
--------------------------------------------------
CONF ERROR: unilm-large-cased config can not be loaded
Message: Unrecognized model in unilm-large-cased. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: t5, distilbert, albert, camembert, xlm-roberta, bart, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl
==================================================

276: Summary for voidful/albert_chinese_base
--------------------------------------------------
TOK ERROR: voidful/albert_chinese_base tokenizer can not be loaded
Message: Model name 'voidful/albert_chinese_base' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'voidful/albert_chinese_base' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

277: Summary for voidful/albert_chinese_large
--------------------------------------------------
TOK ERROR: voidful/albert_chinese_large tokenizer can not be loaded
Message: Model name 'voidful/albert_chinese_large' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'voidful/albert_chinese_large' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

278: Summary for voidful/albert_chinese_small
--------------------------------------------------
TOK ERROR: voidful/albert_chinese_small tokenizer can not be loaded
Message: Model name 'voidful/albert_chinese_small' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'voidful/albert_chinese_small' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

279: Summary for voidful/albert_chinese_tiny
--------------------------------------------------
TOK ERROR: voidful/albert_chinese_tiny tokenizer can not be loaded
Message: Model name 'voidful/albert_chinese_tiny' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'voidful/albert_chinese_tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

280: Summary for voidful/albert_chinese_xlarge
--------------------------------------------------
TOK ERROR: voidful/albert_chinese_xlarge tokenizer can not be loaded
Message: Model name 'voidful/albert_chinese_xlarge' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'voidful/albert_chinese_xlarge' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

281: Summary for voidful/albert_chinese_xxlarge
--------------------------------------------------
TOK ERROR: voidful/albert_chinese_xxlarge tokenizer can not be loaded
Message: Model name 'voidful/albert_chinese_xxlarge' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'voidful/albert_chinese_xxlarge' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

282: Summary for voidism/12to6_distilbert
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

283: Summary for wietsedv/bert-base-dutch-cased
--------------------------------------------------
--------------------------------------------------
PAD in Tokenizer and Config not equal for wietsedv/bert-base-dutch-cased!
PAD in Tokenizer: 3 | PAD in Config: 0
--------------------------------------------------
Config needs change!
==================================================

284: Summary for xlm-clm-ende-1024
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

285: Summary for xlm-clm-enfr-1024
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

286: Summary for xlm-mlm-100-1280
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

287: Summary for xlm-mlm-17-1280
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

288: Summary for xlm-mlm-en-2048
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

289: Summary for xlm-mlm-ende-1024
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

290: Summary for xlm-mlm-enfr-1024
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

291: Summary for xlm-mlm-enro-1024
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

292: Summary for xlm-mlm-tlm-xnli15-1024
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

293: Summary for xlm-mlm-xnli15-1024
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

294: Summary for xlm-roberta-base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

295: Summary for xlm-roberta-large
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

296: Summary for xlm-roberta-large-finetuned-conll02-dutch
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

297: Summary for xlm-roberta-large-finetuned-conll02-spanish
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

298: Summary for xlm-roberta-large-finetuned-conll03-english
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

299: Summary for xlm-roberta-large-finetuned-conll03-german
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

300: Summary for xlnet-base-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

301: Summary for xlnet-large-cased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

302: Summary for yihanlin/scibert_scivocab_uncased
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

303: Summary for yosuke/bert-base-japanese-char
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

304: Summary for dslim23/bert-base-cased-NER-conll-2003
--------------------------------------------------
TOK ERROR: dslim23/bert-base-cased-NER-conll-2003 tokenizer can not be loaded
Message: Model name 'dslim23/bert-base-cased-NER-conll-2003' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'dslim23/bert-base-cased-NER-conll-2003' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

305: Summary for dslim/bert-base-NER
--------------------------------------------------
TODO_1: In dslim/bert-base-NER the eos_token_ids has to be removed
TODO_2: In dslim/bert-base-NER eos_token_ids is 0 but default eos_token_id is None - Adapt on AWS
--------------------------------------------------
EOS in Tokenizer and Config not equal for dslim/bert-base-NER!
EOS in Tokenizer: None |EOS in Config: 0
--------------------------------------------------
BOS in Tokenizer and Config not equal for dslim/bert-base-NER!
BOS in Tokenizer: None | BOS in Config: 0
--------------------------------------------------
Config needs change!
==================================================

306: Summary for mrm8488/bert-tiny-4-finetuned-squadv2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

307: Summary for beomi/EduBERT-small-500-v1
--------------------------------------------------
TODO_1: In beomi/EduBERT-small-500-v1 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for beomi/EduBERT-small-500-v1!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

308: Summary for mrm8488/bert-tiny-5-finetuned-squadv2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

309: Summary for mrm8488/bert-mini-5-finetuned-squadv2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

310: Summary for mrm8488/bert-medium-finetuned-squadv2
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

311: Summary for oya163/NepBERT
--------------------------------------------------
TODO_1: In oya163/NepBERT the eos_token_ids has to be removed
TODO_2: In oya163/NepBERT eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for oya163/NepBERT!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for oya163/NepBERT!
EOS in Tokenizer: 2 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for oya163/NepBERT!
BOS in Tokenizer: 0 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

312: Summary for beomi/EduBERT-small-v2
--------------------------------------------------
TODO_1: In beomi/EduBERT-small-v2 the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for beomi/EduBERT-small-v2!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

313: Summary for gsarti/scibert-nli
--------------------------------------------------
TODO_1: In gsarti/scibert-nli the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for gsarti/scibert-nli!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

314: Summary for deepset/quora_dedup_bert_base
--------------------------------------------------
TODO_1: In deepset/quora_dedup_bert_base the eos_token_ids has to be removed
--------------------------------------------------
PAD in Tokenizer and Config not equal for deepset/quora_dedup_bert_base!
PAD in Tokenizer: 0 | PAD in Config: None
--------------------------------------------------
Config needs change!
==================================================

315: Summary for mrm8488/chEMBL_smiles_v1
--------------------------------------------------
TODO_1: In mrm8488/chEMBL_smiles_v1 the eos_token_ids has to be removed
TODO_2: In mrm8488/chEMBL_smiles_v1 eos_token_ids is None but default eos_token_id is 2 - Adapt on AWS
--------------------------------------------------
PAD in Tokenizer and Config not equal for mrm8488/chEMBL_smiles_v1!
PAD in Tokenizer: 1 | PAD in Config: None
--------------------------------------------------
EOS in Tokenizer and Config not equal for mrm8488/chEMBL_smiles_v1!
EOS in Tokenizer: 2 |EOS in Config: None
--------------------------------------------------
BOS in Tokenizer and Config not equal for mrm8488/chEMBL_smiles_v1!
BOS in Tokenizer: 0 | BOS in Config: None
--------------------------------------------------
Config needs change!
==================================================

316: Summary for pertschuk/albert-intent-model-v3
--------------------------------------------------
TOK ERROR: pertschuk/albert-intent-model-v3 tokenizer can not be loaded
Message: Model name 'pertschuk/albert-intent-model-v3' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'pertschuk/albert-intent-model-v3' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
==================================================

317: Summary for deepset/covid_bert_base
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

318: Summary for dbmdz/bert-base-multilingual-cased-finetuned-conll03-dutch
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

319: Summary for dbmdz/bert-base-multilingual-cased-finetuned-conll03-spanish
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

320: Summary for remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization
--------------------------------------------------
TOK ERROR: remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization tokenizer can not be loaded
Message: Model name 'remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

321: Summary for remi/bertabs-finetuned-extractive-abstractive-summarization
--------------------------------------------------
TOK ERROR: remi/bertabs-finetuned-extractive-abstractive-summarization tokenizer can not be loaded
Message: Model name 'remi/bertabs-finetuned-extractive-abstractive-summarization' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'remi/bertabs-finetuned-extractive-abstractive-summarization' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

322: Summary for remi/bertabs-finetuned-xsum-extractive-abstractive-summarization
--------------------------------------------------
TOK ERROR: remi/bertabs-finetuned-xsum-extractive-abstractive-summarization tokenizer can not be loaded
Message: Model name 'remi/bertabs-finetuned-xsum-extractive-abstractive-summarization' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'remi/bertabs-finetuned-xsum-extractive-abstractive-summarization' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
==================================================

323: Summary for albert-base-v1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

324: Summary for albert-large-v1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

325: Summary for albert-xlarge-v1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================

326: Summary for albert-xxlarge-v1
--------------------------------------------------
--------------------------------------------------
All good!
==================================================
